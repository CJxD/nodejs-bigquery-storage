// Copyright 2022 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     https://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//
// ** This file is automatically generated by gapic-generator-typescript. **
// ** https://github.com/googleapis/gapic-generator-typescript **
// ** All changes to this file may be overwritten. **

/* global window */
import * as gax from 'google-gax';
import {
  Callback,
  CallOptions,
  Descriptors,
  ClientOptions,
  GoogleError,
} from 'google-gax';

import {PassThrough} from 'stream';
import * as protos from '../../protos/protos';
import jsonProtos = require('../../protos/protos.json');
/**
 * Client JSON configuration object, loaded from
 * `src/v1/big_query_write_client_config.json`.
 * This file defines retry strategy and timeouts for all API methods in this library.
 */
import * as gapicConfig from './big_query_write_client_config.json';

import version = require('../../../package.json').version;
import {BigQueryWriteClient} = require('@google-cloud/bigquery-storage').v1;
import type = require('@google-cloud/bigquery-storage').protos.google.protobuf
    .FieldDescriptorProto.Type;
import mode = require('@google-cloud/bigquery-storage').protos.google.cloud
    .bigquery.storage.v1.WriteStream.Type;

/**
 *  BigQuery Write API.
 *
 *  The Write API can be used to write data to BigQuery.
 *
 *  For supplementary information about the Write API, see:
 *  https://cloud.google.com/bigquery/docs/write-api
 * @class
 * @memberof v1
 */

export class WriterClient extends BigQueryWriteClient{
    constructor (opts?: ClientOptions) {
        super();
        const something: string = "something";
    }
    
    async appendRowsPending(writerSchema: string[], serializedRows: string[], parent: string[]) {
        try {
            // create stream
            const writer = new BigQueryWriteClient();
            let writeStream = {type: mode.PENDING};

            let request = {
                parent,
                writeStream,
            };

            let [response] = await writer.createWriteStream(request);

            console.log(`Stream created: ${response.name}`);

            writeStream = response.name;
            // prepare message for sending with headers
            const options: ClientOptions = {};
            options.otherArgs = {};
            options.otherArgs.headers = {};
            options.otherArgs.headers[
              'x-goog-request-params'
            ] = `write_stream=${writeStream}`;
      
            // Append data to the given stream.
            const stream = await writer.appendRows(options);
      
            const responses = [];
      
            stream.on('data', response => {
              // Check for errors.
              if (response.error) {
                throw new Error(response.error.message);
              }
      
              console.log(response);
              responses.push(response);
      
              // Close the stream when all responses have been received.
              if (responses.length == serializedRows.length) {
                stream.end();
              }
            });
      
            stream.on('error', err => {
              throw err;
            });
      
            stream.on('end', async () => {
              // API call completed.
              try {
                [response] = await writer.finalizeWriteStream({
                  name: writeStream,
                });
                console.log(`Row count: ${response.rowCount}`);
      
                [response] = await writer.batchCommitWriteStreams({
                  parent,
                  writeStreams: [writeStream],
                });
                console.log(response);
              } catch (err) {
                console.log(err);
              }
            });
      
        // maybe abstract into a different method
            // send message and receive responses
        let offsetValue = 0;
        
        // create request
            request = {
                writeStream,
                protoRows: {value: serializedRows},
                offset: {value: offsetValue},
              };
            //send batch
            stream.write(request);
        // close stream logic in on 'end' callback above
        } catch (err) {
            console.log(err)
        }
    }

    createWriterSchema() {
        // json object to basic Proto Message
        // return writerSchema;
    }

    createParent(projectId: string, datasetId: string, tableId: string): string {
        const parent = `projects/${projectId}/datasets/${datasetId}/tables/${tableId}`;
        return parent;
    }

    createSerializedRows(rowData) {
      let serializedRows = []
      rowData.forEach(entry => {
        let row = // new Row() instance;
        row.prop = "value";
        row.setValue = "value";
        serializedRows.push(row.serializeBinary())
      })
      return serializedRows;
    }

}

// Example
/*
const writer = new WriterClient()

const projectId = "my-project";
const datasetId = "my-dataset";
const tableId = "my-table";
const parent = writer.createParent(projectId, datasetId, tableId);

const serializedRows = writer.createSerializedRows([]);
const writerSchema = writer.createWriterSchema(args);

writer.appendRowsPending(writerSchema, serializedRows, parent);
*/

// APPENDIX

// Copyright 2022 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//      http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

'use strict';

function main(
  projectId = 'my_project',
  datasetId = 'my_dataset',
  tableId = 'my_table'
) {
  // [START bigquerystorage_append_rows_pending]
  const {BigQueryWriteClient} = require('@google-cloud/bigquery-storage').v1;
  const customer_record_pb = require('./customer_record_pb.js');

  const type = require('@google-cloud/bigquery-storage').protos.google.protobuf
    .FieldDescriptorProto.Type;
  const mode = require('@google-cloud/bigquery-storage').protos.google.cloud
    .bigquery.storage.v1.WriteStream.Type;

  async function appendRowsPending() {
    /**
     * If you make updates to the sample_data.proto protocol buffers definition,
     * run:
     *   protoc --js_out=import_style=commonjs,binary:. customer_record.proto
     * from the /samples directory to generate the customer_record_pb module.
     */

    const writeClient = new BigQueryWriteClient();

    // So that BigQuery knows how to parse the serialized_rows, create a
    // protocol buffer representation of your message descriptor.
    const protoDescriptor = {};
    protoDescriptor.name = 'CustomerRecord';
    protoDescriptor.field = [
      {
        name: 'customer_name',
        number: 1,
        type: type.TYPE_STRING,
      },
    ];

    /**
     * TODO(developer): Uncomment the following lines before running the sample.
     */
    projectId = 'my_project';
    datasetId = 'my_dataset';
    tableId = 'my_table';

    const parent = `projects/${projectId}/datasets/${datasetId}/tables/${tableId}`;

    try {
      // Create a write stream to the given table.
      let writeStream = {type: mode.PENDING};

      let request = {
        parent,
        writeStream,
      };

      let [response] = await writeClient.createWriteStream(request);

      console.log(`Stream created: ${response.name}`);

      writeStream = response.name;

      // This header is required so that the BigQuery Storage API
      // knows which region to route the request to.
      const options = {};
      options.otherArgs = {};
      options.otherArgs.headers = {};
      options.otherArgs.headers[
        'x-goog-request-params'
      ] = `write_stream=${writeStream}`;

      // Append data to the given stream.
      const stream = await writeClient.appendRows(options);

      const responses = [];

      stream.on('data', response => {
        // Check for errors.
        if (response.error) {
          throw new Error(response.error.message);
        }

        console.log(response);
        responses.push(response);

        // Close the stream when all responses have been received.
        if (responses.length === 2) {
          stream.end();
        }
      });

      stream.on('error', err => {
        throw err;
      });

      stream.on('end', async () => {
        // API call completed.
        try {
          [response] = await writeClient.finalizeWriteStream({
            name: writeStream,
          });
          console.log(`Row count: ${response.rowCount}`);

          [response] = await writeClient.batchCommitWriteStreams({
            parent,
            writeStreams: [writeStream],
          });
          console.log(response);
        } catch (err) {
          console.log(err);
        }
      });

      let serializedRows = [];

      // Row 1
      let row = new customer_record_pb.CustomerRecord();
      row.row_num = 1;
      row.setCustomerName('Octavia');
      serializedRows.push(row.serializeBinary());

      // Row 2
      row = new customer_record_pb.CustomerRecord();
      row.row_num = 2;
      row.setCustomerName('Turing');
      serializedRows.push(row.serializeBinary());

      let protoRows = {
        writerSchema: {protoDescriptor},
        rows: {serializedRows},
      };

      // Set an offset to allow resuming this stream if the connection breaks.
      // Keep track of which requests the server has acknowledged and resume the
      // stream at the first non-acknowledged message. If the server has already
      // processed a message with that offset, it will return an ALREADY_EXISTS
      // error, which can be safely ignored.

      // The first request must always have an offset of 0.
      let offsetValue = 0;

      // Construct request.
      request = {
        writeStream,
        protoRows,
        offset: {value: offsetValue},
      };

      // Send batch.
      stream.write(request);

      serializedRows = [];

      // Row 3
      row.row_num = 3;
      row.setCustomerName('bell');
      serializedRows.push(row.serializeBinary());
      protoRows = {
        rows: {serializedRows},
      };

      // Offset must equal the number of rows that were previously sent.
      offsetValue = 2;

      request = {
        protoRows,
        offset: {value: offsetValue},
      };

      // Send batch.
      stream.write(request);
    } catch (err) {
      console.log(err);
    }
  }
  // [END bigquerystorage_append_rows_pending]
  appendRowsPending();
}
process.on('unhandledRejection', err => {
  console.error(err.message);
  process.exitCode = 1;
});
main(...process.argv.slice(2));

// Request message for `AppendRows`.
//
// Due to the nature of AppendRows being a bidirectional streaming RPC, certain
// parts of the AppendRowsRequest need only be specified for the first request
// sent each time the gRPC network connection is opened/reopened.
//
// The size of a single AppendRowsRequest must be less than 10 MB in size.
// Requests larger than this return an error, typically `INVALID_ARGUMENT`.
message AppendRowsRequest {
  // ProtoData contains the data rows and schema when constructing append
  // requests.
  message ProtoData {
    // Proto schema used to serialize the data.  This value only needs to be
    // provided as part of the first request on a gRPC network connection,
    // and will be ignored for subsequent requests on the connection.
    ProtoSchema writer_schema = 1;

    // Serialized row data in protobuf message format.
    // Currently, the backend expects the serialized rows to adhere to
    // proto2 semantics when appending rows, particularly with respect to
    // how default values are encoded.
    ProtoRows rows = 2;
  }

  // Required. The write_stream identifies the target of the append operation, and only
  // needs to be specified as part of the first request on the gRPC connection.
  // If provided for subsequent requests, it must match the value of the first
  // request.
  //
  // For explicitly created write streams, the format is:
  //
  // * `projects/{project}/datasets/{dataset}/tables/{table}/streams/{id}`
  //
  // For the special default stream, the format is:
  //
  // * `projects/{project}/datasets/{dataset}/tables/{table}/streams/_default`.
  string write_stream = 1 [
    (google.api.field_behavior) = REQUIRED,
    (google.api.resource_reference) = {
      type: "bigquerystorage.googleapis.com/WriteStream"
    }
  ];

  // If present, the write is only performed if the next append offset is same
  // as the provided value. If not present, the write is performed at the
  // current end of stream. Specifying a value for this field is not allowed
  // when calling AppendRows for the '_default' stream.
  google.protobuf.Int64Value offset = 2;

  // Input rows. The `writer_schema` field must be specified at the initial
  // request and currently, it will be ignored if specified in following
  // requests. Following requests must have data in the same format as the
  // initial request.
  oneof rows {
    // Rows in proto format.
    ProtoData proto_rows = 4;
  }

  // Id set by client to annotate its identity. Only initial request setting is
  // respected.
  string trace_id = 6;
}

// Response message for `AppendRows`.
message AppendRowsResponse {
  // AppendResult is returned for successful append requests.
  message AppendResult {
    // The row offset at which the last append occurred. The offset will not be
    // set if appending using default streams.
    google.protobuf.Int64Value offset = 1;
  }

  oneof response {
    // Result if the append is successful.
    AppendResult append_result = 1;

    // Error returned when problems were encountered.  If present,
    // it indicates rows were not accepted into the system.
    // Users can retry or continue with other append requests within the
    // same connection.
    //
    // Additional information about error signalling:
    //
    // ALREADY_EXISTS: Happens when an append specified an offset, and the
    // backend already has received data at this offset.  Typically encountered
    // in retry scenarios, and can be ignored.
    //
    // OUT_OF_RANGE: Returned when the specified offset in the stream is beyond
    // the current end of the stream.
    //
    // INVALID_ARGUMENT: Indicates a malformed request or data.
    //
    // ABORTED: Request processing is aborted because of prior failures.  The
    // request can be retried if previous failure is addressed.
    //
    // INTERNAL: Indicates server side error(s) that can be retried.
    google.rpc.Status error = 2;
  }

  // If backend detects a schema update, pass it to user so that user can
  // use it to input new type of message. It will be empty when no schema
  // updates have occurred.
  TableSchema updated_schema = 3;

  // If a request failed due to corrupted rows, no rows in the batch will be
  // appended. The API will return row level error info, so that the caller can
  // remove the bad rows and retry the request.
  repeated RowError row_errors = 4;

  // The target of the append operation. Matches the write_stream in the
  // corresponding request.
  string write_stream = 5;
}