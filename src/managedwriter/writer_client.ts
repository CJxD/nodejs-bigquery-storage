// Copyright 2022 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     https://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//
// ** This file is automatically generated by gapic-generator-typescript. **
// ** https://github.com/googleapis/gapic-generator-typescript **
// ** All changes to this file may be overwritten. **

/* global window */
import '../../package.json';
import * as gax from 'google-gax';
import * as protobufjs from 'protobufjs';
import type {CallOptions, ClientOptions} from 'google-gax';
import * as protos from '../../protos/protos';
// import jsonProtos = require('../../protos/protos.json');

import {BigQueryWriteClient} from '../v1';

/**
 *  BigQuery Write API.
 *
 *  The Write API can be used to write data to BigQuery.
 *
 *  For supplementary information about the Write API, see:
 *  https://cloud.google.com/bigquery/docs/write-api
 * @class
 * @memberof storage
 */

type WriteStream = protos.google.cloud.bigquery.storage.v1.IWriteStream;
type AppendRowResponse =
  protos.google.cloud.bigquery.storage.v1.AppendRowsResponse;
type AppendRowRequest =
  protos.google.cloud.bigquery.storage.v1.IAppendRowsRequest;
type IInt64Value = protos.google.protobuf.IInt64Value;
type ProtoData =
  protos.google.cloud.bigquery.storage.v1.AppendRowsRequest.IProtoData;
type ProtoRows = protos.google.cloud.bigquery.storage.v1.IProtoRows;
type ProtoSchema = protos.google.cloud.bigquery.storage.v1.IProtoSchema;
type ProtoDescriptor = protos.google.protobuf.IDescriptorProto;
type Message = gax.protobuf.Message;

export class WriterClient {
  private _opts: ClientOptions | undefined;
  private _parent: string;
  private _writeStream: WriteStream = {type: 'PENDING'};
  private _streamId: string;
  private _client: BigQueryWriteClient;

  constructor(
    parent: string,
    client?: BigQueryWriteClient,
    opts?: ClientOptions,
    writeStream?: WriteStream
  ) {
    this._parent = parent;
    this._client = new BigQueryWriteClient(opts) || client;
    this._writeStream = writeStream || this._writeStream;
    this._streamId = 'Please open a connection to set connection name';
    // Ensure that options include all the required fields.
    const staticMembers = this.constructor as typeof WriterClient;
  }

  setParent = (projectId: string, datasetId: string, tableId: string): void => {
    const parent = `projects/${projectId}/datasets/${datasetId}/tables/${tableId}`;
    this._parent = parent;
  };

  getParent = (): string => {
    return this._parent;
  };

  getStreamId = (): string => {
    return this._streamId;
  };

  getClient = (): BigQueryWriteClient => {
    return this._client;
  };

  setClient = (client: BigQueryWriteClient): void => {
    this._client = client;
  };

  createSerializedRows = (
    rowData: Message[],
    schema?: ProtoSchema
  ): ProtoData => {
    const serializedRows: ProtoRows = {
      serializedRows: [],
    };
    const convertedRows: Uint8Array[] = [];
    rowData.forEach(entry => {
      convertedRows.push(protobufjs.Message.encode(entry).finish());
    });
    serializedRows.serializedRows = convertedRows;
    const protoDescriptor: ProtoDescriptor = {};
    const writerSchema: ProtoSchema = schema || {
      protoDescriptor: protoDescriptor,
    };
    const serializedRowData: ProtoData = {
      rows: serializedRows,
      writerSchema: writerSchema,
    };
    return serializedRowData;
  };

  getWriteStreams = (writeStream: WriteStream): undefined | null | string[] => {
    if (writeStream === undefined || writeStream.name === undefined) {
      return undefined;
    }
    if (writeStream.name === null) {
      return null;
    }
    return new Array(writeStream.name);
  };

  setWriteStream(streamType: WriteStream): void {
    this._writeStream = {type: streamType.type};
  }

  getWriteStream(): WriteStream {
    return this._writeStream;
  }

  async initializeStreamConnection(
    clientOptions?: CallOptions
  ): Promise<gax.CancellableStream> {
    const request: protos.google.cloud.bigquery.storage.v1.ICreateWriteStreamRequest =
      {
        parent: this.getParent(),
        writeStream: this.getWriteStream(),
      };
    console.log(`This is the initializeStreamConnection request: ${request}`);
    const [response] = await this._client.createWriteStream(request);
    if (typeof [response] === undefined) {
      throw new gax.GoogleError(`${response}`);
    }
    console.log(`Stream connection created: ${response.name}`);
    try {
      if (response.name) {
        this._streamId = response.name;
        console.log('This is the response name and what was set as streamId');
        console.log(response.name);
        console.log(this._streamId);
      }
    } catch {
      throw new Error('Stream connection failed');
    }
    if (clientOptions) {
      return this._client.appendRows(clientOptions);
    }
    return this._client.appendRows();
  }

  async appendRowsToStream(
    connection: gax.CancellableStream,
    serializedRows: ProtoData,
    offsetValue: IInt64Value
  ): Promise<AppendRowResponse[]> {
    const responses: AppendRowResponse[] | null = [];
    const writeStream: string = this.getStreamId();
    console.log(`This is the write_stream string we're using: ${writeStream}`);

    const request: AppendRowRequest = {
      writeStream: writeStream,
      protoRows: serializedRows,
      offset: offsetValue,
    };

    connection.write(request, () => {
      console.log('Write complete!');
      connection.on('data', response => {
        // Check for errors.
        if (response.error) {
          throw new Error(response.error.message);
        }

        console.log(
          `This is the response we're getting over the wire from an appendRows request: ${response}`
        );
        responses.push(response);
        console.log(...responses);
        console.log(responses.length);

        if (responses.length === 1) {
          connection.end(() => {
            console.log(...responses);
            this.closeStream();
            console.log('The connection has ended.');
          });
        }
      });

      connection.on('error', err => {
        throw err;
      });
    });
    return responses;
  }

  async closeStream(): Promise<void> {
    // API call completed.
    const writeStream = this._streamId;
    console.log(
      `This is the writeStream we've stored and will be committing ${writeStream}`
    );
    const writeStreams: string[] | null = [];
    writeStreams.push(writeStream);
    console.log(
      `This is the array of writeStreams we're committing ${writeStreams}`
    );

    const finalizeStreamReq: protos.google.cloud.bigquery.storage.v1.IFinalizeWriteStreamRequest =
      {
        name: writeStream,
      };
    console.log(
      `This is the FinalizeWriteStreamRequest object: ${finalizeStreamReq}`
    );
    this._client.finalizeWriteStream(finalizeStreamReq).then(result => {
      if (!result.includes(undefined)) {
        const [validResponse] = result;
        console.log(`Row count: ${validResponse.rowCount}`);
      }
    });
    const batchCommitWriteStreamsReq: protos.google.cloud.bigquery.storage.v1.IBatchCommitWriteStreamsRequest =
      {
        parent: this._parent,
        writeStreams: writeStreams,
      };
    console.log(
      `This is the BatchCommitWriteStreamsRequest object we're sending: ${batchCommitWriteStreamsReq}`
    );
    this._client
      .batchCommitWriteStreams(batchCommitWriteStreamsReq)
      .then(result => console.log(result));
  }
}

//Example
/*const type = protos.google.protobuf.FieldDescriptorProto.Type;
const customer_record_pb = require('./../../samples/customer_record_pb.js');
const projectId = 'your-project';
const datasetId = 'your-dataset';
const tableId = 'your-table';
const exOpts: ClientOptions = {
  projectId: projectId,
}
const exParent = `projects/${projectId}/datasets/${datasetId}/tables/${tableId}`
const writer = new WriterClient(exParent, exOpts);
const streamType: WriteStream = {
  type: "PENDING"
};
writer.setWriteStream(streamType);
const writeStreamConnection: Promise<gax.CancellableStream> = writer.initializeStreamConnection();

const protoDescriptorEx: ProtoDescriptor = {};
protoDescriptorEx.name = 'CustomerRecord';
protoDescriptorEx.field = [
  {
    name: 'customer_name',
    number: 1,
    type: type.TYPE_STRING,
  },
  {
    name: 'row_num',
    number: 2,
    type: type.TYPE_INT64
  }
    ];

// Row 1
let row1Message = new customer_record_pb.CustomerRecord();
row1Message.row_num = 1
row1Message.setCustomerName("Octavia")

// Row 2
let row2Message = new customer_record_pb.CustomerRecord();
row2Message.row_num = 2;
row2Message.setCustomerName("Turing");

const writerSchemaEx: ProtoSchema = {
  protoDescriptor: protoDescriptorEx
}
const serializedRowsEx: ProtoRows = {
  serializedRows: [row1Message.serializeBinary(), row2Message.serializeBinary()]
}

const rowData: ProtoData = {
  rows: serializedRowsEx,
  writerSchema: writerSchemaEx
}
console.log(`This is the length of the rows: ${rowData.rows?.serializedRows?.length}`)

const offset: IInt64Value = {
  value: 0
}
writeStreamConnection.then(res => {
  writer.appendRowsToStream(res, rowData, offset).then(res => {
    console.log(`AppendRowsToStream has resolved: ${res}`);
  });
}).then(() => {
  writer.closeStream().then(res => {
    console.log(`Close stream has been resolved: ${res}`)
  })
});*/
